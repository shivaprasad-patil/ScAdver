# ScAdver: Encoder Training and Query Projection - Quick Summary

## ๐ฏ The Core Concept

**Training Once, Project Forever**

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ PHASE 1: TRAINING (Do Once)                                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ  Reference Data โ [Train Encoder] โ Learned Transformation  โ
โ                                                              โ
โ  What encoder learns:                                       โ
โ  โข "CD3+CD8+ = T-cell" (biology to KEEP)                   โ
โ  โข "Library size = batch effect" (noise to REMOVE)         โ
โ                                                              โ
โ  Result: ~6M weights encode these rules                     โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
         โ Freeze weights โ๏ธ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ PHASE 2: PROJECTION (Repeat โ times)                        โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ  Query Batch 1 โ [Frozen Encoder] โ Batch-corrected Zโ      โ
โ  Query Batch 2 โ [Frozen Encoder] โ Batch-corrected Zโ      โ
โ  Query Batch 3 โ [Frozen Encoder] โ Batch-corrected Zโ      โ
โ                                                              โ
โ  Same transformation, no training!                          โ
โ  Fast: < 1 second per batch                                 โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```

---

## ๐ฌ How Training Works (Reference Data)

### The Adversarial Setup

```python
for epoch in range(500):
    # 1. Encode
    Z = encoder(X_reference)
    
    # 2. Three objectives compete
    
    # โ Biology Classifier (wants to succeed)
    bio_pred = bio_classifier(Z)
    bio_loss = CrossEntropy(bio_pred, true_celltypes)
    # โ Encoder learns to KEEP biological patterns
    
    # โ Batch Discriminator (encoder wants it to fail)
    batch_pred = batch_discriminator(Z)
    batch_loss = -CrossEntropy(batch_pred, true_batches)  # Negative!
    # โ Encoder learns to REMOVE batch patterns
    
    # โ Decoder (reconstruction)
    X_recon = decoder(Z)
    recon_loss = MSE(X_reference, X_recon)
    # โ Encoder learns to preserve information
    
    # 3. Combined objective
    total_loss = recon_loss + 20.0*bio_loss - 0.5*batch_loss
    encoder.update(total_loss)
```

### What Gets Learned

The encoder learns a **non-linear filter**:

| Input Pattern | Encoder Decision | Why |
|--------------|------------------|-----|
| CD3+CD8+ genes | โ **Keep in Z** | Bio-classifier needs this for T-cell prediction |
| CD19+CD20+ genes | โ **Keep in Z** | Bio-classifier needs this for B-cell prediction |
| High library size | โ **Remove from Z** | Batch-discriminator would use this |
| Protocol artifacts | โ **Remove from Z** | Batch-discriminator would use this |
| Cell cycle genes | โ **Keep in Z** | Biological variation (not batch) |

These decisions are encoded in **~6 million weights** across 4 layers.

---

## ๐ง How Freezing Works

```python
# After training
model.eval()                    # Disable dropout, batch norm updates
for param in model.parameters():
    param.requires_grad = False # Disable gradient computation

# Weights are now FIXED
# No optimizer, no backward pass, no updates
```

The transformation `f(X) = Encoder(X)` is now **deterministic and unchanging**.

---

## ๐ How Query Projection Works

### No Training Happens

```python
# Query data (NEW batch: smartseq2)
X_query = [cell1, cell2, ..., cell1000]

# Forward pass only (no training)
with torch.no_grad():
    Z_query = frozen_encoder(X_query)
    # Takes < 1 second for 1000 cells
```

### Why Batch Correction Happens Automatically

**The encoder already knows what to do:**

```
Query T-cell:
  Input: [CD3: 8.5, CD8: 7.2, LibrarySize: 15000, ...]
         โ
  Layer 1: "CD3+CD8 = important pattern" (learned from reference)
         โ
  Layer 2: "High library size = ignore" (learned from reference)
         โ
  Layer 3: "Combine T-cell markers" (learned from reference)
         โ
  Layer 4: Output Z in "T-cell region" (learned from reference)
         โ
  Result: Batch-free T-cell embedding!
```

**Key insight:** The encoder learned **general rules** during training:
- "What patterns = biology?" (gene combinations for cell types)
- "What patterns = batch?" (technical noise, library size, etc.)

These rules apply to **any data** with similar biology, regardless of batch.

---

## ๐ Mathematical Intuition

### Training Phase: Learn a Function

The encoder learns a function `f: โโฟ โ โแต` where:
- `n` = number of genes (e.g., 2000)
- `d` = latent dimension (e.g., 256)

This function is optimized such that:
```
f(X) = Z where:
  - Z contains biological information (bio_classifier succeeds)
  - Z lacks batch information (batch_discriminator fails)
```

### Projection Phase: Apply the Function

For new query data:
```
X_query โ f(X_query) โ Z_query
```

Since `f` was optimized to:
- Extract biology-related features
- Ignore batch-related features

It does the same for query data automatically!

**Analogy to PCA:**
```
PCA: Learn linear projection W that maximizes variance
     Apply W to new data โ same transformed space

ScAdver: Learn non-linear projection f that maximizes biology, minimizes batch
         Apply f to new data โ same transformed space
```

---

## โ Why Biology Is Preserved

### During Training
```python
# Bio-classifier forces encoder to retain cell type info
bio_pred = bio_classifier(encoder(X))
loss = CrossEntropy(bio_pred, true_celltypes)
weight = 20.0  # HIGH weight!

# If encoder loses biological info:
#   โ bio_classifier accuracy drops
#   โ loss increases (ร20!)
#   โ encoder gets huge gradient penalty
#   โ encoder learns to PRESERVE biology
```

### During Projection
```python
# Same biological patterns โ Same encoder response

Reference T-cell: CD3+CD8+ โ encoder โ Region A (T-cell cluster)
Query T-cell: CD3+CD8+ โ encoder โ Region A (same cluster!)

Reference B-cell: CD19+CD20+ โ encoder โ Region B (B-cell cluster)
Query B-cell: CD19+CD20+ โ encoder โ Region B (same cluster!)
```

**The biological variation creates distinct regions in Z, and this structure is preserved for query data.**

---

## โ Why Batch Effects Are Removed

### During Training
```python
# Batch-discriminator tries to predict batches
batch_pred = batch_discriminator(encoder(X))
loss = -CrossEntropy(batch_pred, true_batches)  # NEGATIVE!
weight = 0.5

# If encoder keeps batch info:
#   โ batch_discriminator succeeds
#   โ loss becomes very negative
#   โ encoder gets gradient to REMOVE batch info
#   โ encoder learns to HIDE batches
```

### During Projection
```python
# Query batch patterns โ Ignored by encoder (learned behavior)

Reference (batch1): High library size โ encoder filters out โ Z
Reference (batch2): Low library size โ encoder filters out โ Z
Query (smartseq2): Different library โ encoder filters out โ Z (same space!)

# Batch patterns don't affect Z because encoder learned to ignore them
```

---

## ๐ฏ Your Use Case: Fixed Reference + Multiple Query Batches

### Workflow

```python
# Step 1: Train ONCE on reference
adata_ref_corrected, model, metrics = adversarial_batch_correction(
    adata=adata_reference,  # 10,000 cells, multiple batches
    bio_label='celltype',
    batch_label='tech',
    epochs=500              # ~10-30 minutes
)

# Step 2: Save model
torch.save(model.state_dict(), 'scadver_model.pt')

# Step 3: Process queries as they arrive (NO retraining!)
adata_query1 = transform_query(model, query_batch_1)  # < 1 sec
adata_query2 = transform_query(model, query_batch_2)  # < 1 sec
adata_query3 = transform_query(model, query_batch_3)  # < 1 sec
```

### Benefits

| Aspect | Retraining Each Time | Using transform_query |
|--------|---------------------|----------------------|
| **Speed** | 10 min ร 3 = 30 min | < 3 seconds total |
| **Consistency** | Different models โ incompatible embeddings | Same model โ compatible embeddings |
| **Bias** | Query affects training | Query doesn't affect model |
| **Scalability** | Slow with many queries | Fast with unlimited queries |
| **Storage** | Need to retrain each time | Save model once, reuse forever |

---

## ๐ Key Takeaways

1. **Encoder learns general transformation during training**
   - Biology patterns โ Keep
   - Batch patterns โ Remove
   
2. **Weights encode this transformation (~6M parameters)**
   - Fixed after training
   - Generalizes to new data
   
3. **Query projection applies same transformation**
   - No training needed
   - Fast (< 1 second)
   - Automatic batch correction
   - Automatic biology preservation
   
4. **Works because of generalization**
   - Encoder learned patterns, not memorized cells
   - Same biological patterns in query โ Same encoder response
   - Same batch-like patterns in query โ Filtered out automatically

---

## ๐ Verification

Check that it works:

```python
# After projection, verify integration
adata_combined = sc.concat([adata_ref, adata_query])
sc.pp.neighbors(adata_combined, use_rep='X_ScAdver')
sc.tl.umap(adata_combined)

# Check 1: Batch mixing (should be mixed)
sc.pl.umap(adata_combined, color='batch')
# โ Different batches mixed together

# Check 2: Biology preserved (should cluster)
sc.pl.umap(adata_combined, color='celltype')
# โ Cell types form distinct clusters

# Check 3: Metrics
from sklearn.metrics import silhouette_score
batch_sil = silhouette_score(Z, batches)  # Lower is better
bio_sil = silhouette_score(Z, celltypes)   # Higher is better
```

---

## ๐ Further Reading

- **Detailed mechanism**: See [ENCODER_MECHANISM_EXPLAINED.md](ENCODER_MECHANISM_EXPLAINED.md)
- **Code example**: See [examples/incremental_query_example.py](examples/incremental_query_example.py)
- **Visual diagrams**: Run `python examples/visualize_encoder_mechanism.py`

---

## ๐ก Bottom Line

**The encoder is like a smart filter that learned how to separate signal (biology) from noise (batch). Once trained, this filter applies to any new data automatically. No retraining needed!**

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ  Training: Learn what's signal vs noise         โ
โ  Projection: Apply learned filter to new data   โ
โ  Result: Batch-free, biology-rich embeddings!   โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```
